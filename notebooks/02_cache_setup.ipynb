{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Cache Setup\n",
        "\n",
        "**Objective:**  \n",
        "Create a script to hold session data and serve rows for streaming.\n",
        "\n",
        "**Instructions:**\n",
        "- Load CSV/Parquet/JSON file into a Python cache (list/dict/DataFrame)\n",
        "- Validate: Can iterate/stream rows one-by-one, data freshness (row reflects realistic event order)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Add project root to Python path\n",
        "# In Jupyter, getcwd() typically returns the project root\n",
        "# If not, navigate up from notebooks/ directory\n",
        "current_dir = os.getcwd()\n",
        "if os.path.basename(current_dir) == 'notebooks':\n",
        "    # We're in notebooks/ directory, go up one level\n",
        "    project_root = os.path.dirname(current_dir)\n",
        "else:\n",
        "    # We're already at project root\n",
        "    project_root = current_dir\n",
        "\n",
        "# Add to path if not already there\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from src.cache_manager import CacheManager\n",
        "from src.utils import validate_data\n",
        "\n",
        "print(\"✅ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cache manager initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize cache manager\n",
        "cache_manager = CacheManager()\n",
        "print(\"✅ Cache manager initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.cache_manager:Loading data from ../data/2023_Monaco_Race_20251119_182219.parquet (parquet format)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using data file: 2023_Monaco_Race_20251119_182219.parquet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.cache_manager:Loaded 982330 records\n",
            "INFO:src.utils:Calculated frequency: 999848759.83 Hz (avg interval: 0.00 ms)\n",
            "INFO:src.cache_manager:Cache loaded successfully. Frequency: 999848759.83 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data loaded into cache: 982330 records\n",
            "✅ Frequency: 999848759.83 Hz\n"
          ]
        }
      ],
      "source": [
        "# Find latest data file\n",
        "data_dir = \"../data\"\n",
        "data_files = [f for f in os.listdir(data_dir) if f.endswith(('.csv', '.parquet'))]\n",
        "\n",
        "if not data_files:\n",
        "    print(\"❌ No data files found. Please run Step 1 first.\")\n",
        "else:\n",
        "    # Use latest file\n",
        "    latest_file = max(data_files, key=lambda f: os.path.getctime(os.path.join(data_dir, f)))\n",
        "    file_path = os.path.join(data_dir, latest_file)\n",
        "    print(f\"Using data file: {latest_file}\")\n",
        "    \n",
        "    # Load data into cache\n",
        "    format_type = 'parquet' if latest_file.endswith('.parquet') else 'csv'\n",
        "    if cache_manager.load_cache(file_path, format=format_type):\n",
        "        print(f\"✅ Data loaded into cache: {cache_manager.get_num_records()} records\")\n",
        "        print(f\"✅ Frequency: {cache_manager.get_frequency():.2f} Hz\")\n",
        "    else:\n",
        "        print(\"❌ Failed to load data into cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.cache_manager:Cache reset to beginning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing row-by-row iteration...\n",
            "Record 1: Driver=VER, Speed=0.0\n",
            "Record 2: Driver=ALO, Speed=0.0\n",
            "Record 3: Driver=LEC, Speed=0.0\n",
            "Record 4: Driver=SAI, Speed=0.0\n",
            "Record 5: Driver=HUL, Speed=0.0\n",
            "\n",
            "✅ Cache iteration successful: 5 records retrieved\n"
          ]
        }
      ],
      "source": [
        "# Validate cache can iterate row-by-row\n",
        "print(\"Testing row-by-row iteration...\")\n",
        "sample_records = []\n",
        "for i in range(5):  # Test first 5 records\n",
        "    record = cache_manager.get_next_record()\n",
        "    if record:\n",
        "        sample_records.append(record)\n",
        "        print(f\"Record {i+1}: Driver={record.get('DriverID', 'N/A')}, Speed={record.get('Speed', 'N/A')}\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "if sample_records:\n",
        "    print(f\"\\n✅ Cache iteration successful: {len(sample_records)} records retrieved\")\n",
        "    # Reset to beginning\n",
        "    cache_manager.reset()\n",
        "else:\n",
        "    print(\"❌ Cache iteration failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating data order (timestamps)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.cache_manager:Cache reset to beginning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data is ordered chronologically (realistic event order)\n",
            "First timestamp: 1970-01-01 00:00:00.000003722\n",
            "Last timestamp: 1970-01-01 00:00:00.000010334\n",
            "Duration: 0 days 00:00:00.000006612\n",
            "\n",
            "✅ Step 2 Complete: Cache setup successful!\n"
          ]
        }
      ],
      "source": [
        "# Validate data order (timestamps should be increasing)\n",
        "print(\"Validating data order (timestamps)...\")\n",
        "all_records = cache_manager.get_all_records()\n",
        "\n",
        "if 'SessionTime' in all_records.columns:\n",
        "    # Handle timedelta or datetime types\n",
        "    timestamps = all_records['SessionTime']\n",
        "    \n",
        "    # Check if it's already timedelta or datetime\n",
        "    if pd.api.types.is_timedelta64_dtype(timestamps):\n",
        "        # Already timedelta, use directly\n",
        "        is_ordered = (timestamps.diff().dropna() >= pd.Timedelta(0)).all()\n",
        "        first_ts = timestamps.iloc[0]\n",
        "        last_ts = timestamps.iloc[-1]\n",
        "        duration = last_ts - first_ts\n",
        "    elif pd.api.types.is_datetime64_any_dtype(timestamps):\n",
        "        # Already datetime, use directly\n",
        "        is_ordered = (timestamps.diff().dropna() >= pd.Timedelta(0)).all()\n",
        "        first_ts = timestamps.iloc[0]\n",
        "        last_ts = timestamps.iloc[-1]\n",
        "        duration = last_ts - first_ts\n",
        "    else:\n",
        "        # Try to convert - check if it's timedelta string format\n",
        "        if timestamps.dtype == 'object' and timestamps.astype(str).str.contains('days', na=False).any():\n",
        "            timestamps = pd.to_timedelta(timestamps)\n",
        "        else:\n",
        "            # Try datetime first, then timedelta\n",
        "            try:\n",
        "                timestamps = pd.to_datetime(timestamps)\n",
        "            except (TypeError, ValueError):\n",
        "                timestamps = pd.to_timedelta(timestamps)\n",
        "        \n",
        "        is_ordered = (timestamps.diff().dropna() >= pd.Timedelta(0)).all()\n",
        "        first_ts = timestamps.iloc[0]\n",
        "        last_ts = timestamps.iloc[-1]\n",
        "        duration = last_ts - first_ts\n",
        "    \n",
        "    if is_ordered:\n",
        "        print(\"✅ Data is ordered chronologically (realistic event order)\")\n",
        "    else:\n",
        "        print(\"⚠️ Warning: Some timestamps are out of order\")\n",
        "    \n",
        "    print(f\"First timestamp: {first_ts}\")\n",
        "    print(f\"Last timestamp: {last_ts}\")\n",
        "    print(f\"Duration: {duration}\")\n",
        "else:\n",
        "    print(\"⚠️ Warning: SessionTime column not found, cannot validate order\")\n",
        "\n",
        "# Reset cache\n",
        "cache_manager.reset()\n",
        "print(\"\\n✅ Step 2 Complete: Cache setup successful!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
